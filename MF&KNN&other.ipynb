{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains test results on three Baseline, MF-ALS, MF-SGD, NNMF and KNN: user-based and item based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from helpers import *\n",
    "from utils import *\n",
    "from mf import *\n",
    "from blend import *\n",
    "from helpers import *\n",
    "from baselines import *\n",
    "# from utils import split_data\n",
    "\n",
    "# from baselines import *\n",
    "# from matrix_factorization import matrix_factorization_sgd, write_sgd_prediction, matrix_factorization_als, write_als_prediction\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process data and traform to sparse matrix\n",
    "path_dataset = \"./data/data_train.csv\"\n",
    "ratings = load_data(path_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of original ratings. (# of row, # of col): (10000, 1000)\n",
      "the shape of valid ratings. (# of row, # of col): (9990, 999)\n",
      "Total number of nonzero elements in origial data:1176952\n",
      "Total number of nonzero elements in train data:1065253\n",
      "Total number of nonzero elements in test data:111620\n"
     ]
    }
   ],
   "source": [
    "#load data and store in pickle \n",
    "_, train, test = split_data(ratings, 10)\n",
    "# with open('./data/pickle/train.pickle', 'wb') as file:\n",
    "#     pickle.dump(train, file)\n",
    "# with open('./data/pickle/test.pickle', 'wb') as file:\n",
    "#     pickle.dump(test, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to use global mean, user mean and item mean to test the error of baseline model. It is reasonable that these model do not have good performace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Global Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of baseline using the global mean: [[1.12152228]].\n",
      "--- 9.212589025497437 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#to test the result of global mean\n",
    "start_time = time.time()\n",
    "global_mean_rmse = baseline_global_mean(train, test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction mode...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "baseline_global_mean(ratings, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 User Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of the baseline using the user mean: [[1.03317038]].\n",
      "--- 304.16880893707275 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "user_mean_rmse = baseline_user_mean(train, test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction mode...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "baseline_user_mean(ratings, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Item Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE of the baseline using the item mean: [[1.09633198]].\n",
      "--- 852.9861969947815 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "user_mean_rmse = baseline_item_mean(train, test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction mode...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "baseline_item_mean(ratings, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Matrix Facrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tune the hyper-parameters for matrix factorisation with SGD, a k-fold cross validation is used, with $k$ set to be 5. By using grid search, the item penalisation coefficient $\\lambda_{it} = 0.25$, the user penalisation coefficient $\\lambda_{us} = 0.01$, the latent variable $k=20$. This process of SGD is iterated by 50 times, when the change between iterations are small enough to be ignored. The running time of this method is 2067s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.039124803056718.\n",
      "iter: 1, RMSE on training set: 1.0093596412816275.\n",
      "iter: 2, RMSE on training set: 0.996118185715355.\n",
      "iter: 3, RMSE on training set: 0.9889373788680876.\n",
      "iter: 4, RMSE on training set: 0.9833487188730043.\n",
      "iter: 5, RMSE on training set: 0.9791512984503469.\n",
      "iter: 6, RMSE on training set: 0.9751336656193352.\n",
      "iter: 7, RMSE on training set: 0.9717590789067294.\n",
      "iter: 8, RMSE on training set: 0.9687342408761761.\n",
      "iter: 9, RMSE on training set: 0.9664820588574188.\n",
      "iter: 10, RMSE on training set: 0.964651459302041.\n",
      "iter: 11, RMSE on training set: 0.9630064750838252.\n",
      "iter: 12, RMSE on training set: 0.9617396096478493.\n",
      "iter: 13, RMSE on training set: 0.9606984146043444.\n",
      "iter: 14, RMSE on training set: 0.9598885581372865.\n",
      "iter: 15, RMSE on training set: 0.9591781749595529.\n",
      "iter: 16, RMSE on training set: 0.95862109786241.\n",
      "iter: 17, RMSE on training set: 0.9581442446142858.\n",
      "iter: 18, RMSE on training set: 0.9578401463973725.\n",
      "iter: 19, RMSE on training set: 0.9575394522537206.\n",
      "iter: 20, RMSE on training set: 0.9572988344647946.\n",
      "iter: 21, RMSE on training set: 0.9570919757122619.\n",
      "iter: 22, RMSE on training set: 0.9569505514867123.\n",
      "iter: 23, RMSE on training set: 0.9568238807689141.\n",
      "iter: 24, RMSE on training set: 0.9567074167396636.\n",
      "iter: 25, RMSE on training set: 0.9566171333413002.\n",
      "iter: 26, RMSE on training set: 0.9565376427347564.\n",
      "iter: 27, RMSE on training set: 0.9564738248680222.\n",
      "iter: 28, RMSE on training set: 0.9564210078912891.\n",
      "iter: 29, RMSE on training set: 0.9563771615794339.\n",
      "iter: 30, RMSE on training set: 0.9563414470031959.\n",
      "iter: 31, RMSE on training set: 0.9563110386281031.\n",
      "iter: 32, RMSE on training set: 0.9562861939567485.\n",
      "iter: 33, RMSE on training set: 0.956265279117519.\n",
      "iter: 34, RMSE on training set: 0.9562476033566305.\n",
      "iter: 35, RMSE on training set: 0.956233089950573.\n",
      "iter: 36, RMSE on training set: 0.9562209500548489.\n",
      "iter: 37, RMSE on training set: 0.9562108480403082.\n",
      "iter: 38, RMSE on training set: 0.9562024472088333.\n",
      "iter: 39, RMSE on training set: 0.9561954432174515.\n",
      "iter: 40, RMSE on training set: 0.9561896125737306.\n",
      "iter: 41, RMSE on training set: 0.9561847518973289.\n",
      "iter: 42, RMSE on training set: 0.9561806995263066.\n",
      "iter: 43, RMSE on training set: 0.9561773290711927.\n",
      "iter: 44, RMSE on training set: 0.9561745196291278.\n",
      "iter: 45, RMSE on training set: 0.9561721722738699.\n",
      "iter: 46, RMSE on training set: 0.9561702192675212.\n",
      "iter: 47, RMSE on training set: 0.9561685900826591.\n",
      "iter: 48, RMSE on training set: 0.956167231681415.\n",
      "iter: 49, RMSE on training set: 0.9561661014643722.\n",
      "---Training is done---\n",
      "test RMSE after running SGD:: 1.002519674033661.\n",
      "--- 4183.572791099548 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9561661014643722, 1.002519674033661)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_rmse, test_rmse = matrix_factorization_SGD(train, test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "train_rmse, test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0371469403325468.\n",
      "iter: 1, RMSE on training set: 1.0058452983067736.\n",
      "iter: 2, RMSE on training set: 0.9971213239404874.\n",
      "iter: 3, RMSE on training set: 0.9899612636873218.\n",
      "iter: 4, RMSE on training set: 0.9856546557359349.\n",
      "iter: 5, RMSE on training set: 0.9807428461103578.\n",
      "iter: 6, RMSE on training set: 0.9765817920128849.\n",
      "iter: 7, RMSE on training set: 0.9732945816838856.\n",
      "iter: 8, RMSE on training set: 0.9705215877872423.\n",
      "iter: 9, RMSE on training set: 0.9679279799052876.\n",
      "iter: 10, RMSE on training set: 0.9660294750254609.\n",
      "iter: 11, RMSE on training set: 0.9642347435570328.\n",
      "iter: 12, RMSE on training set: 0.9628158756423747.\n",
      "iter: 13, RMSE on training set: 0.9615922257456738.\n",
      "iter: 14, RMSE on training set: 0.9607294205498229.\n",
      "iter: 15, RMSE on training set: 0.9599863589892276.\n",
      "iter: 16, RMSE on training set: 0.9593782954408286.\n",
      "iter: 17, RMSE on training set: 0.9589287163489776.\n",
      "iter: 18, RMSE on training set: 0.9585168210865638.\n",
      "iter: 19, RMSE on training set: 0.9582283734988002.\n",
      "iter: 20, RMSE on training set: 0.9579966665584984.\n",
      "iter: 21, RMSE on training set: 0.957794756251395.\n",
      "iter: 22, RMSE on training set: 0.957631487382482.\n",
      "iter: 23, RMSE on training set: 0.9574980149263036.\n",
      "iter: 24, RMSE on training set: 0.9573862907468395.\n",
      "iter: 25, RMSE on training set: 0.9572850717452233.\n",
      "iter: 26, RMSE on training set: 0.9572026498184707.\n",
      "iter: 27, RMSE on training set: 0.9571367593515715.\n",
      "iter: 28, RMSE on training set: 0.9570821275314665.\n",
      "iter: 29, RMSE on training set: 0.9570361753074251.\n",
      "iter: 30, RMSE on training set: 0.9569979856745932.\n",
      "iter: 31, RMSE on training set: 0.9569669722702289.\n",
      "iter: 32, RMSE on training set: 0.9569406381009676.\n",
      "iter: 33, RMSE on training set: 0.9569185636433276.\n",
      "iter: 34, RMSE on training set: 0.9569004063546311.\n",
      "iter: 35, RMSE on training set: 0.9568853076090839.\n",
      "iter: 36, RMSE on training set: 0.9568726007131554.\n",
      "iter: 37, RMSE on training set: 0.9568620892353544.\n",
      "iter: 38, RMSE on training set: 0.9568533114069591.\n",
      "iter: 39, RMSE on training set: 0.9568460242547597.\n",
      "iter: 40, RMSE on training set: 0.9568399167976355.\n",
      "iter: 41, RMSE on training set: 0.9568348372328022.\n",
      "iter: 42, RMSE on training set: 0.9568306003122463.\n",
      "iter: 43, RMSE on training set: 0.956827075694341.\n",
      "iter: 44, RMSE on training set: 0.95682413405912.\n",
      "iter: 45, RMSE on training set: 0.9568216843801186.\n",
      "iter: 46, RMSE on training set: 0.956819643939324.\n",
      "iter: 47, RMSE on training set: 0.9568179422935816.\n",
      "iter: 48, RMSE on training set: 0.9568165228753764.\n",
      "iter: 49, RMSE on training set: 0.9568153411343494.\n",
      "---Training is done---\n",
      "Start prediction mode...\n",
      "Predicting Now:  ./data/sample_submission.csv\n",
      "Predicting file is created.\n",
      "--- 8017.506133794785 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9568153411343494"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_rmse, _ = matrix_factorization_SGD(ratings, None)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same idea as SGD, matrix factorisation with ALS is tuned by using grid search as well. After setting $k=20$, which is found at the initial cursory grid search, it is observed that most results of ALS are better than SGD. Hence, its parameter optimisation is investigated more precisely with a finer grid in grid search. below shows the grid search plot, where the brightest area indicates the most precise prediction. The best-tuned model found turns out to have the item penalisation coefficient $\\lambda_{it} = 0.575$, the user penalisation coefficient $\\lambda_{us} = 0.014$. The model is trained such that the change of improvement between each iteration is neglectable ($10^{-6}$). The running time of this method is 1847s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the ALS algorithm...\n",
      "RMSE on training set: 1.7988331125449577.\n",
      "RMSE on training set: 1.227674094817536.\n",
      "RMSE on training set: 1.09014290374031.\n",
      "RMSE on training set: 1.0215641182849295.\n",
      "RMSE on training set: 0.9821347184405147.\n",
      "RMSE on training set: 0.9591860578835757.\n",
      "RMSE on training set: 0.9450930193009871.\n",
      "RMSE on training set: 0.9358935292624367.\n",
      "RMSE on training set: 0.929529564867519.\n",
      "RMSE on training set: 0.9249122418289314.\n",
      "RMSE on training set: 0.9214368661278576.\n",
      "RMSE on training set: 0.918745941429446.\n",
      "RMSE on training set: 0.9166152996841797.\n",
      "RMSE on training set: 0.9148973513099954.\n",
      "RMSE on training set: 0.9134910830735328.\n",
      "RMSE on training set: 0.9123251800090414.\n",
      "RMSE on training set: 0.9113479887502243.\n",
      "RMSE on training set: 0.9105212718565168.\n",
      "RMSE on training set: 0.9098161781173688.\n",
      "RMSE on training set: 0.9092105656215611.\n",
      "RMSE on training set: 0.9086871799429747.\n",
      "RMSE on training set: 0.9082323885053672.\n",
      "RMSE on training set: 0.9078352853387438.\n",
      "RMSE on training set: 0.9074870474290005.\n",
      "RMSE on training set: 0.9071804648904136.\n",
      "RMSE on training set: 0.9069095930573319.\n",
      "RMSE on training set: 0.9066694912866058.\n",
      "RMSE on training set: 0.9064560242424099.\n",
      "RMSE on training set: 0.906265708777496.\n",
      "RMSE on training set: 0.9060955945049892.\n",
      "RMSE on training set: 0.9059431695758539.\n",
      "RMSE on training set: 0.9058062855553545.\n",
      "RMSE on training set: 0.9056830969554347.\n",
      "RMSE on training set: 0.9055720121556556.\n",
      "RMSE on training set: 0.9054716532788076.\n",
      "RMSE on training set: 0.9053808231852943.\n",
      "RMSE on training set: 0.9052984781810519.\n",
      "RMSE on training set: 0.9052237053485266.\n",
      "RMSE on training set: 0.9051557036441498.\n",
      "RMSE on training set: 0.9050937680803637.\n",
      "RMSE on training set: 0.9050372764453799.\n",
      "RMSE on training set: 0.9049856781162258.\n",
      "RMSE on training set: 0.9049384846053381.\n",
      "RMSE on training set: 0.904895261544352.\n",
      "RMSE on training set: 0.9048556218625233.\n",
      "RMSE on training set: 0.9048192199610126.\n",
      "RMSE on training set: 0.9047857467183883.\n",
      "RMSE on training set: 0.9047549251920901.\n",
      "RMSE on training set: 0.9047265069048217.\n",
      "RMSE on training set: 0.9047002686232153.\n",
      "RMSE on training set: 0.9046760095536417.\n",
      "RMSE on training set: 0.9046535488913791.\n",
      "RMSE on training set: 0.9046327236720424.\n",
      "RMSE on training set: 0.9046133868810169.\n",
      "RMSE on training set: 0.904595405785807.\n",
      "RMSE on training set: 0.9045786604600752.\n",
      "RMSE on training set: 0.9045630424751225.\n",
      "RMSE on training set: 0.9045484537358599.\n",
      "RMSE on training set: 0.9045348054450371.\n",
      "RMSE on training set: 0.9045220171780606.\n",
      "RMSE on training set: 0.9045100160568496.\n",
      "RMSE on training set: 0.9044987360101062.\n",
      "RMSE on training set: 0.9044881171105489.\n",
      "RMSE on training set: 0.9044781049805698.\n",
      "RMSE on training set: 0.9044686502581257.\n",
      "---Training is done---\n",
      "test RMSE after running ALS: 0.9865100643060458.\n",
      "--- 12413.087761878967 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9044686502581257, 0.9865100643060458)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_rmse, test_rmse = matrix_factorization_ALS(train, test)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "train_rmse, test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn the matrix factorization using SGD...\n",
      "iter: 0, RMSE on training set: 1.0371469403325468.\n",
      "iter: 1, RMSE on training set: 1.0058452983067736.\n",
      "iter: 2, RMSE on training set: 0.9971213239404874.\n",
      "iter: 3, RMSE on training set: 0.9899612636873218.\n",
      "iter: 4, RMSE on training set: 0.9856546557359349.\n",
      "iter: 5, RMSE on training set: 0.9807428461103578.\n",
      "iter: 6, RMSE on training set: 0.9765817920128849.\n",
      "iter: 7, RMSE on training set: 0.9732945816838856.\n",
      "iter: 8, RMSE on training set: 0.9705215877872423.\n",
      "iter: 9, RMSE on training set: 0.9679279799052876.\n",
      "iter: 10, RMSE on training set: 0.9660294750254609.\n",
      "iter: 11, RMSE on training set: 0.9642347435570328.\n",
      "iter: 12, RMSE on training set: 0.9628158756423747.\n",
      "iter: 13, RMSE on training set: 0.9615922257456738.\n",
      "iter: 14, RMSE on training set: 0.9607294205498229.\n",
      "iter: 15, RMSE on training set: 0.9599863589892276.\n",
      "iter: 16, RMSE on training set: 0.9593782954408286.\n",
      "iter: 17, RMSE on training set: 0.9589287163489776.\n",
      "iter: 18, RMSE on training set: 0.9585168210865638.\n",
      "iter: 19, RMSE on training set: 0.9582283734988002.\n",
      "iter: 20, RMSE on training set: 0.9579966665584984.\n",
      "iter: 21, RMSE on training set: 0.957794756251395.\n",
      "iter: 22, RMSE on training set: 0.957631487382482.\n",
      "iter: 23, RMSE on training set: 0.9574980149263036.\n",
      "iter: 24, RMSE on training set: 0.9573862907468395.\n",
      "iter: 25, RMSE on training set: 0.9572850717452233.\n",
      "iter: 26, RMSE on training set: 0.9572026498184707.\n",
      "iter: 27, RMSE on training set: 0.9571367593515715.\n",
      "iter: 28, RMSE on training set: 0.9570821275314665.\n",
      "iter: 29, RMSE on training set: 0.9570361753074251.\n",
      "iter: 30, RMSE on training set: 0.9569979856745932.\n",
      "iter: 31, RMSE on training set: 0.9569669722702289.\n",
      "iter: 32, RMSE on training set: 0.9569406381009676.\n",
      "iter: 33, RMSE on training set: 0.9569185636433276.\n",
      "iter: 34, RMSE on training set: 0.9569004063546311.\n",
      "iter: 35, RMSE on training set: 0.9568853076090839.\n",
      "iter: 36, RMSE on training set: 0.9568726007131554.\n",
      "iter: 37, RMSE on training set: 0.9568620892353544.\n",
      "iter: 38, RMSE on training set: 0.9568533114069591.\n",
      "iter: 39, RMSE on training set: 0.9568460242547597.\n",
      "iter: 40, RMSE on training set: 0.9568399167976355.\n",
      "iter: 41, RMSE on training set: 0.9568348372328022.\n",
      "iter: 42, RMSE on training set: 0.9568306003122463.\n",
      "iter: 43, RMSE on training set: 0.956827075694341.\n",
      "iter: 44, RMSE on training set: 0.95682413405912.\n",
      "iter: 45, RMSE on training set: 0.9568216843801186.\n",
      "iter: 46, RMSE on training set: 0.956819643939324.\n",
      "iter: 47, RMSE on training set: 0.9568179422935816.\n",
      "iter: 48, RMSE on training set: 0.9568165228753764.\n",
      "iter: 49, RMSE on training set: 0.9568153411343494.\n",
      "---Training is done---\n",
      "Start prediction mode...\n",
      "Predicting Now:  ./data/sample_submission.csv\n",
      "Predicting file is created.\n",
      "--- 2200.569477081299 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9568153411343494"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "train_rmse, _ = matrix_factorization_SGD(ratings, None)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "train_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NNMF section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Neural network matrix factorization` or NNMF, for short—dominates standard low-rank techniques on a suite of benchmark but is dominated by some recent proposals that take advantage of the graph features. Given the vast range of architectures, activation functions, regularizers, and optimizationtechniques that could be used within the NNMF framework, it seems likely the true potential of the approach has yet to be reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is presented by Gintare Karolina Dziugaite.The neural network contains three layers with 50 units. \n",
    "After tuning hyper-parameter, we set lamda=1.4841, D=40, D_prim=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import nnmf.nnmf \n",
    "import nnmf.predict\n",
    "import nnmf.split_data  \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in data\n",
      "number of items: 10000, number of users: 1000\n",
      "Data subsets:\n",
      "Train: 953330\n",
      "Validation: 105926\n",
      "Test: 117696\n"
     ]
    }
   ],
   "source": [
    "#split train and test set by our defaut setting\n",
    "nnmf.split_data.split_nnmf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building network & initializing variables\n",
      "Reading in data\n",
      "[start] Train error: 95769.312500, Train RMSE: 1.823475; Valid RMSE: 1.820590\n",
      "Early stopping (0.9900772571563721 vs. 0.9904701709747314)...\n",
      "Loading best checkpointed model\n",
      "./model/nnmf.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ./model/nnmf.ckpt\n",
      "Final train RMSE: 0.958625078201294\n",
      "Final test RMSE: 0.9892654418945312\n"
     ]
    }
   ],
   "source": [
    "#training nnmf (test_ratio=0.1)\n",
    "#if you want to see the process, set verbose=True\n",
    "nnmf.nnmf.do_nnmf(mode='train', verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a prediction algorithm that computes a prediction for the rating exploiting a weighted sum of the other users/items ratings with the help of a similarity metric, in our case Pearson Baseline. This algorithm implemented in the Python Surprise library. min_k (int) – The minimum number of neighbors to take into account for aggregation. If there are not enough neighbors, the prediction is set the the global mean of all ratings. Default is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import BaselineOnly\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import KNNBasic\n",
    "import os\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read and set train and test in surprise library\n",
    "file_path_train = os.path.expanduser('./data/mov_kaggle.all')\n",
    "reader = Reader(line_format='user item rating', sep='\\t')\n",
    "data = Dataset.load_from_file(file_path_train, reader=reader)\n",
    "trainset, testset = train_test_split(data, test_size=.1)   #test ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## user_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.019656231639403"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'msd',\n",
    "               'user_based': True  # compute  similarities between items\n",
    "               }\n",
    "algo = KNNBasic(k = 80, sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## item_based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0234965094325155"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_options = {'name': 'msd',\n",
    "               'user_based': False  # compute  similarities between users\n",
    "               }\n",
    "algo = KNNBasic(k = 20, sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
